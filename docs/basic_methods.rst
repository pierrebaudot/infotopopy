How to Use InfoTopo
===================

Infotopo is a general Machine Learning set of tools gathering Topology 
(Cohomology and Homotopy), statistics and information theory 
(information quantifies statistical structures generically) and 
statistical physics.
It provides a matheamticaly formalised expression of deep network and learning,
and propose anuspervised or supervised learning mode (as a special case of the first).
It allows a simple and autonatic exploration of the data structures, dimension reduction
and supervised or unsupervised classification.

The raw methods are computationnally consuming due to the intrinsic combinatorial 
nature of the topological tools, even in the simplest case of a simplicial case 
(the general case is based on the much broader partition combinatorics) the 
computational complexity is of the order of :math:`2^n` .
As a consequence, an important part of the tools and methods are dedicated 
to overcome this extensive computation. Among the possible strategies and 
heuristics used or currently developped, are:

_ restrict to simplicial cohomology and combinatorics (done here).

_ possible exploration of only the low dimensional structures (done here).

_ possible exploration of only most or least informative paths (done here).

_ possible restriction 2nd degree-dimension statistical interactions: 
what is computed here is the equivalent of the Cech complex (with all degree-
dimension computed), and such restriction is equivalent to computing the Vietoris-Rips 
complex (in development). 

_ compute on GPU (in development).

As a result, for this 0.1 version of the software, and for computation with 
commercial average PC, we recommand to analyse up to 20 variables (or dimensions)
at a time in the raw brut-force approach (see performance section).



We now present some basic example of use, inspiring our presentation from 
the remarkable presentation of `UMAP by McInnes. <https://umap-learn.readthedocs.io/en/latest/>`_
We first import some few tools: some of the datasets available in sklearn, seaborn to
visualise the results, and pandas to handle the data.

.. code:: python3

    from sklearn.datasets import load_iris, load_digits, load_boston, load_diabetes
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    import timeit

.. code:: python3

    sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})

Iris data
---------

Iris dataset
~~~~~~~~~~~~

The first example of dataset application we will present is the `iris
dataset <https://en.wikipedia.org/wiki/Iris_flower_data_set>`__. It is
a very small dataset composed of 4 Random-Variables or dimensions that 
quantify various petals and sepals observables of 3 different species of 
Iris flowers, like petal length, for 150 flowers or points (50 for each 
species). In the context of Infotopo it means that dimension_tot = 4  
and sample_size = 150 (we consider all the points), and as the dimension
of the data set is small we will make the complete analysis of the 
simplicial structure of dependencies by setting the maximum dimension 
of analysis to dimension_max = dimension_tot. We also set the other 
parameters of infotopo to approriate, as further explained.   
We can load the iris dataset from sklearn.

.. code:: python3

    iris = load_iris()
    iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)
    print(iris.DESCR)

    dimension_max = iris.data.shape[1]
    dimension_tot = iris.data.shape[1]
    sample_size = iris.data.shape[0]
    nb_of_values =9
    forward_computation_mode = False
    work_on_transpose = False
    supervised_mode = False
    sampling_mode = 1
    deformed_probability_mode = False
    


.. parsed-literal::

    Iris Plants Database
    ====================
    
    Notes
    -----
    Data Set Characteristics:
        :Number of Instances: 150 (50 in each of three classes)
        :Number of Attributes: 4 numeric, predictive attributes and the class
        :Attribute Information:
            - sepal length in cm
            - sepal width in cm
            - petal length in cm
            - petal width in cm
            - class:
                    - Iris-Setosa
                    - Iris-Versicolour
                    - Iris-Virginica
        :Summary Statistics:
    
        ============== ==== ==== ======= ===== ====================
                        Min  Max   Mean    SD   Class Correlation
        ============== ==== ==== ======= ===== ====================
        sepal length:   4.3  7.9   5.84   0.83    0.7826
        sepal width:    2.0  4.4   3.05   0.43   -0.4194
        petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
        petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)
        ============== ==== ==== ======= ===== ====================
    
        :Missing Attribute Values: None
        :Class Distribution: 33.3% for each of 3 classes.
        :Creator: R.A. Fisher
        :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
        :Date: July, 1988
    
    This is a copy of UCI ML iris datasets.
    http://archive.ics.uci.edu/ml/datasets/Iris
    
    The famous Iris database, first used by Sir R.A Fisher
    
    This is perhaps the best known database to be found in the
    pattern recognition literature.  Fisher's paper is a classic in the field and
    is referenced frequently to this day.  (See Duda & Hart, for example.)  The
    data set contains 3 classes of 50 instances each, where each class refers to a
    type of iris plant.  One class is linearly separable from the other 2; the
    latter are NOT linearly separable from each other.
    
    References
    ----------
       - Fisher,R.A. "The use of multiple measurements in taxonomic problems"
         Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
         Mathematical Statistics" (John Wiley, NY, 1950).
       - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.
         (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.
       - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
         Structure and Classification Rule for Recognition in Partially Exposed
         Environments".  IEEE Transactions on Pattern Analysis and Machine
         Intelligence, Vol. PAMI-2, No. 1, 67-71.
       - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
         on Information Theory, May 1972, 431-433.
       - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
         conceptual clustering system finds 3 classes in the data.
       - Many, many more ...
    

As visualizing data in 4 dimensions or more is hard or not possible, we can first 
plot all the pairwise scatterplot matrix to present the pairwise correlations and 
dependencies between the variables, using Seaborn and pandas dataframe.

.. code:: python3

    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
    iris_df['species'] = pd.Series(iris.target).map(dict(zip(range(3),iris.target_names)))
    sns.pairplot(iris_df, hue='species')
    plt.show()


.. image:: images/iris_pairwise_scatter.png


All those 2D views gives a rought but misleading idea of what the data looks 
like in high dimensions since, as we will see, some fully emergent  
statistical dependences (called synergy in the original work of Bialek's team) can appear in higher dimension which are 
totally unobservable in those 2D views. However such 2D views gives a fair
visual estimation of how much each pairs of variale covary, the correlation 
coefficient and its generalization to non-linear relations, the pairwise 
Mutual Information (I2). In Topological Data Analysis (TDA) terms, it gives rought 
idea of what the skeleton of a Vietoris-Rips (information or correlation) complex
of the data could be.
We will see how to go beyond this pairwise statistical interaction case, and how
we can unravel some purely emergent higher dimensional interations. Along this 
way, we will see how to compute and estimate all classical information functions,
multivariate Entropies, Mutual Informations and Conditional Entropies and 
Mutual Informations. 

Entropy
~~~~~~~

To use infotopo we need to first construct a infotopo object from 
the infotopo package. This makes a lot of same word, information is a 
functor, a kind of general application or map, that could be either a 
function or a class. So let's first import the infotopo library, we a set 
of specifications of the parametters (cf. section parameters, some of them 
like dimension_max = dimension_tot and sample_size have been fixed 
previously to the size of the data input matrix).

.. code:: python3

    import infotopo

.. code:: python3

    information_topo = infotopo.infotopo(dimension_max = dimension_max, 
                                dimension_tot = dimension_tot, 
                                sample_size = sample_size, 
                                work_on_transpose = work_on_transpose,
                                nb_of_values = nb_of_values, 
                                sampling_mode = sampling_mode, 
                                deformed_probability_mode = deformed_probability_mode,
                                supervised_mode = supervised_mode, 
                                forward_computation_mode = forward_computation_mode)

Now we will compute all the simplicial semi-lattice of marginal and joint-entropy, 
that contains :math:`2^n` elements including the unit 0 reference measure element. 
The marginal :math:`H_1` entopies are defined as classicaly by `Shannon <https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjtrqOXrsPrAhVCrxoKHcBDBrQQgAMoAHoECBIQAg&url=http%3A%2F%2Fscholar.google.fr%2Fscholar_url%3Furl%3Dhttps%3A%2F%2Fpure.mpg.de%2Frest%2Fitems%2Fitem_2383162_7%2Fcomponent%2Ffile_2456978%2Fcontent%26hl%3Dfr%26sa%3DX%26scisig%3DAAGBfm2dgGR4Ly92eRCfhrM1BgCnbIBvBA%26nossl%3D1%26oi%3Dscholarr&usg=AOvVaw0ha99XPEPwgTiv3oMC7PTE>`_ :

.. math::	
    H_1=H(X_{j};P)=k\sum_{x \in [N_j] }p(x)\ln p(x) 

and the multivariate joint-entropies :math:`H_k` just generalise the preceding to k variables:

.. math::	
    H_k= H(X_{1},...,X_{k};P)=  k\sum_{x_1,...,x_k\in [N_1\times...\times N_k]}^{N_1\times...\times N_k}p(x_1.....x_k)\ln p(x_1.....x_k)     

The figure below give the usual Venn diagrams representation of set theoretic unions 
and the corresponding semi-lattice of joint Random Variables and Joint Entropies, together 
with its correponding simplicial representation, for 3 (top) and 4 variables-dimension 
(bottom, the case of the iris dataset with 2 power 4 joint random variables). This correspondence
of joint-information with the semi-lattice of union was formalized by `Hu Kuo Ting <https://drive.google.com/file/d/10ZMjN8Q4w6t4osaYODdBg-pF9zlu21R_/view?usp=sharing>`_ . 
The edges of the lattice are in one to one correspondence with conditional entropies.   

.. image:: images/figure_lattice.png

To do this we will call simplicial_entropies_decomposition, that gives in output 
all the joint entropies in the form of a dictionary with keys given by the tuple of 
the joint variables (ex: (1,3,4)) and  with values the joint or marginal entropy in bit 
(presented below).

.. code:: python3

    Nentropie = information_topo.simplicial_entropies_decomposition(iris.data)


.. parsed-literal::

    {(4,): 2.9528016441309237, (3,): 2.4902608474907497, (2,): 2.5591245822618114, (1,): 2.8298425472847066, (3, 4): 3.983309507504916, (2, 4): 4.798319817958397, (1, 4): 4.83234271597051, (2, 3): 4.437604597473526, (1, 3): 4.2246575340121835, (1, 2): 4.921846615158947, (2, 3, 4): 5.561696151051504, (1, 3, 4): 5.426426190681815, (1, 2, 4): 6.063697650692486, (1, 2, 3): 5.672729631265195, (1, 2, 3, 4): 6.372515544003377}


Such dictionary is hard to read; to allow a relevant visualization of the
the simplicial entropy structure, the function simplicial_entropies_decomposition
also plots the Entropy landscapes. Entropy landscapes provides a representation of the lattice 
of joint (:math:`H_k`) and conditional entropies (noted as the action of Y :math:`Y.H_k`, 
for :math:`H(X_1,...,X_k|Y)`) that ranks the joint variables as a function of their entropy value 
and of the rank-dimensions as illustrated in the figure below:

.. image:: images/Principles_TIDA.png

An Entropy of Information Path is a sequence of inclusive tuples of increasing dimensions and follows 
the edges of the semi-lattice, and the slope of such a path is exactly minus the conditional-entropy, 
as a basic representation of the fundamental chain rule of Entropy.

While the total dimension n (dimension_tot) of the analysis increases, the number of subsets of k 
variables (or k-tuples) increases combinatorially, following the binomial coefficient C(n,k). 
It hence becomes rapidly fully impractical to vizualize, plot and to differentiate the C(n,k) values of 
entropy obtained in dimension k. The Entropy landscapes hence plot the histograms of entropy values as a 
function of the dimension-rank k, and the number of bins of the histograms is imposed by the parameter 
nb_bins_histo. The count of the number of subsets having entropy values in the bin range of the histograms
is represented by a color code in the entropy landscapes. Hence, Entropy Landscapes shall be understood as
(unormalised..but it could be normalised) entropy measure densities histograms (there is interesting further
theoretical and applied development upon this point, since entropy functions obey axioms of measure: one 
could legitamely investigate entropies of entropies, a kind of complexity of information landscapes, see
`Hsu et al. <https://www.mdpi.com/1099-4300/19/10/550>`_ ). 

To plot the Entropy Landscapes and the distribution of entropy values for each dimension-rank k, we use 
the "entropy_simplicial_lanscape" command as following:   

.. code:: python3 

    information_topo.entropy_simplicial_lanscape(Nentropie)

On the example of Iris dataset, the Entropy Landscape we obtain look like this: 

.. image:: images/figure_entropy_simplicial_lattice.png

In this low dimensional case (dimension_tot = 4), the landscapes are very low informative (poor information 
structure) and the histrograms have low meaning, since there is only one subset-k-tuple per bin value, and hence only 
one color (here the green value of 1). The Entropy Landscape themselfs are quite poor in information, joint-entropy is
monotonically increasing along entropy path, a direct consequence of conditional-entropy positivity (concavity argument) 
which is moreover the basic fact at the origin of the basic topological expression of the 2nd law of thermodynamic [3].
As a consequence, we usually do not uncover a lot of usefull information on the datas structure from those Entropy Landscape,
at the exception of curse of dimensionality quantification and in some cases, (assymptotic) entropy rates (to do).
Basically, joint-entropy quantifies "randomness" (in a non formal definition of the word), uncertainty, or how much the 
data points spreads in the dimensions of the variables. Hence low entropies shall be intrepreted as "localised" densities
of data points or sparsness of the probability density histograms (also not in the usual kurtosis sens).  

In any entropy or information function estimation, it is necessary to check that the number of sample is sufficient to 
provide a faithfull estimate, to avoid the sampling problem also called "curse of dimension". The command 
"entropy_simplicial_lanscape" also computes the maximal dimension above which the estimation becomes too inacurate and
shall not be interpreted. This is explained in more details in the section "curse_of_dimension_and_statistical_dependencies_test". 

Mutual Information
~~~~~~~~~~~~~~~~~~

Now, let's have a look at the statistical dependencies structures in the dataset by computing the Mutual-Information lanscapes 
which principle is depicted in the preceding figure and that basically plots k-dimensional multivariate Mutual Informations (:math:`I_k`) in the same 
way as Entropy Landscapes. Pairwise Mutual Information :math:`I_2` is defined as usual following `Shannon <https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjtrqOXrsPrAhVCrxoKHcBDBrQQgAMoAHoECBIQAg&url=http%3A%2F%2Fscholar.google.fr%2Fscholar_url%3Furl%3Dhttps%3A%2F%2Fpure.mpg.de%2Frest%2Fitems%2Fitem_2383162_7%2Fcomponent%2Ffile_2456978%2Fcontent%26hl%3Dfr%26sa%3DX%26scisig%3DAAGBfm2dgGR4Ly92eRCfhrM1BgCnbIBvBA%26nossl%3D1%26oi%3Dscholarr&usg=AOvVaw0ha99XPEPwgTiv3oMC7PTE>`_ :

.. math::	
    I_2=I(X_{1};X_{2};P)=k\sum_{x_1,x_2\in[N_1\times N_2]}^{N_1\times N_2}p(x_1.x_2)\ln \frac{p(x_1)p(x_2)}{p(x_1.x_2)}

They generalize to the multivariate mutual informations, :math:`I_k`, as alternated functions of entropies, following `McGill <https://drive.google.com/file/d/1Cpem9LVFYNScAcihBnqw7IRUjnW-wU04/view?usp=sharing>`_ and `Hu Kuo Ting <https://drive.google.com/file/d/10ZMjN8Q4w6t4osaYODdBg-pF9zlu21R_/view?usp=sharing>`_.

.. math::	
    I_k(X_1,...,X_k;P)=\sum_{i=1}^{k}(-1)^{i-1}\sum_{I\subset [k];card(I)=i}H_i(X_I;P)

For example: 
.. math::
    `I_3=H(X_1)+H(X_2)+H(X_3)-H(X_1,X_2)-H(X1,X_3)-H(X_2,X_3)+H(X_1,X_2,X_3)`:    

`Hu Kuo Ting <https://drive.google.com/file/d/10ZMjN8Q4w6t4osaYODdBg-pF9zlu21R_/view?usp=sharing>`_ showed the correspondence of :math:`I_k` with set intersections
semi-lattice (of finite measurable functions), and we hence have just like with entropy the following information structure, corresponding to intections on Venn 
diagrams: 

.. image:: images/informationfunctions.png

The other functions that quantifies multivariate depence are Total Correlations, :math:`G_k` (`Watanabe <http://www.neuralmachines.com/references/correlation.pdf>`_ , see section diabetes data) 
, or total free energy, or Integrated Information (`Tononi and Edelman <http://www.neuralmachines.com/references/correlation.pdf>`_ ) which are the Kullback-Leibler Divergence between the full joint-entropy and its marginals product, 
for example, :math:`G_3=H(X_1)+H(X_2)+H(X_3)-H(X_1,X_2,X_3)`:

.. math::
    G_k= G_k(X_1;...X_k;P)=\sum_{i=1}^k H(X_i) - H(X_1;...X_k)

Whereas, :math:`G_k` quantifies the total interactions, :math:`I_k` quantify the contribution of the kth interaction. Notably, we have the theorems
that state that n variables are independent if and only if  :math:`G_n =0`, and n variables are independent if and only if  all the :math:`2^n-n-1` 
:math:`I_k` functions with :math:`k \geq 2` are null (e.g. :math:`I_k` provides a refined independence measure `PDF <https://www.mdpi.com/1099-4300/21/9/869>`_). 
In contrast with :math:`G_k`, :math:`I_k` can be negative for :math:`k \geq 3`, a phenomenon called synergy in the original study of Brenner et al.
Considering the old goal of expressing all of physics in terms of information, following Brillouin, Jaynes, Wheeller (...), for `k \geq 2`, 
:math:`G_k` corresponds to a Free-Energy functional of a k interacting body system, while the  :math:`I_k` quantifies the contribution of the 
k-bodies interaction to this total free energy. The :math:`I_1` component is the internal energy:

.. math::
    H_k(X_1,..,X_k;P_N)=E(X_1,..,X_k;P_N)-G(X_1,..,X_k;P_N)=E-G

The Free-energy decomposes itself as an alternated sum of :math:`I_k` : 

.. math::
    G_k =\sum_{i=2}^{k}(-1)^{i}\sum_{I\subset [n];card(I)=i}I_i(X_I;P)    

To plot the Information Landscapes and the distribution of :math:`I_k` values for each dimension-rank k, we use 
the "entropy_simplicial_lanscape" command as following:   

.. code:: python3 

    information_topo.mutual_info_simplicial_lanscape(Ninfomut)

On the example of Iris dataset, the Entropy Landscape we obtain look like this: 

.. image:: images/iris_info_landscapes.png

To obtain the first m k-tuples with maximum and minimum value in dimension k, and if the dimension is 2,3 or 4 plot the data points in the 
corresponding k-subspace (the 4th dimension is represented by a color code), we use the "display_higher_lower_information".
For exmaple, plotting the 2 first maximum and minimum in dimension (framed in red and blue respectively in the last figure), we use 
the following command: 

.. code:: python3 

    information_topo = infotopo(dim_to_rank = 2, number_of_max_val = 2)
    dico_max, dico_min = information_topo.display_higher_lower_information(Ninfomut, dataset)    

On the example of Iris dataset, we obtain the two pairs of variables (3,4) and (1,3) that are the most statistically dependent ("correlated"): 

.. image:: images/iris_max_I2.png

And we obtain the two pairs of variables (1,2) and (2,3) that are the less statistically dependent ("uncorrelated"):

.. image:: images/iris_min_I2.png

Whenever the dimension to study is more than 4, the function only retreives the dictionaries of the first maximum and minimum tuples (to print).

Information Networks
~~~~~~~~~~~~~~~~~~~~

In biology (e.g "omic"), neuroscience (e.g "neural network") and social science (e.g "social network"), it is common and helpfull to conceive and 
visualize the one and two dimensional results as (first degree) networks. To visualize the Information Networks, we use the 
"mutual_info_pairwise_network" as following:

.. code:: python3 

    adjacency_matrix_mut_info = information_topo.mutual_info_pairwise_network(Ninfomut)

The area of each vertex is a function of the marginals information :math:`H_1=I_1` and the thickness of the edges is a function of the pairwise
mutual information or total correlation :math:`I_2=G_2`. On Iris dataset, it gives:  

.. image:: images/iris_info_network.png

The adjacency matrix of information have the marginals informations :math:`H_1=I_1` in its diagonal and is symmetric with respect to the diagonal
as the result of the commutativity of the join-variables and mutual-variables operation in classical information theory (classical is by opposition 
with quantum information theory). Compared to usual distance matrix (with given metric) computed in machine learning (for clustering or classifications),
the  :math:`I_k` are not metric (e.g. non zero diagonal and no triangle inequality), we will introduce to information metric in the next stepps. 
With such Matrix it is possible to apply some usual computational persistence homology tools like `Mapper scikit-tda <https://github.com/scikit-tda>`_ 
(created by Singh, MÃ©moli, and Carlsson) and to build what could be called an "informational Vietoris-Ripps complex". In the context of Morse theory,
information landscapes consider infomation functions themselfs as height or "Morse" functions. However there is likely a much more fundamental application of 
persistence theory in the construction of a local probability density estimation (to be done). :math:`I_k` with :math:`k \geq 3` can be repesented in an 
analgous way using k-cliques as acheived in `Tapia & al 2018 <https://www.nature.com/articles/s41598-018-31765-z>`_ (to be done in the package). They 
shall be represented using k-tensor formalism. In the context of complex networks studies those higher :math:`I_k` with :math:`k \geq 3` correspond to 
hypergraphs or `multiplex or multilayer networks <https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198753919.001.0001/oso-9780198753919>`_
The raw result obtained here is a fully connected network, but one can obtain a sparse matrix and a sparsely connected network by thresholding 
the :math:`I_k` with a with fixed p-value, using the exact statistical dependence test implemented in the package. 

We begin to see that Homology provides a wide generalisation of complex networks (a 1-complex, that is a graph) to higher interactions structures.

Diabetes data
-------------

Diabetes dataset
~~~~~~~~~~~~~~~~

The Iris dataset and its associated information landsacpes are in too low dimension to appreciate all the interest of the methods in higher dimensions,
so lets turn to larger dimensional classical machine learning dataset: Diabetes dataset. This dataset is kindly also furnished by scikitlearn, and we load it with the same methods as previously:

.. code:: python3 

    dataset = load_diabetes()
    dataset_df = pd.DataFrame(dataset.data, columns = dataset.feature_names)
    dimension_max = dataset.data.shape[1]
    dimension_tot = dataset.data.shape[1]
    sample_size = dataset.data.shape[0]
    nb_of_values = 9
    forward_computation_mode = False
    work_on_transpose = False
    supervised_mode = False
    sampling_mode = 1
    deformed_probability_mode = False
    dataset_df = pd.DataFrame(dataset.data, columns=dataset.feature_names)

This dataset contains 10 variables-dimensions for a sample size (number of points) of 442 and a target (label) variable which quantifies diabetes 
progress. The ten variables are [age, sex, body mass index, average blood pressure, T-Cells, low-density lipoproteins, high-density lipoproteins,
thyroid stimulating hormone, lamotrigine, blood sugar level] in this order. 


Entropy
~~~~~~~

As before, we execute:

.. code:: python3

    Nentropie = information_topo.simplicial_entropies_decomposition(iris.data)
    information_topo.entropy_simplicial_lanscape(Nentropie)
    information_topo = infotopo(dim_to_rank = 4, number_of_max_val = 3)
    dico_max, dico_min = information_topo.display_higher_lower_information(Nentropie, dataset)

and we obtain the following entropy landscape:

.. image:: images/diabetes_entropy_landscape.png

which corresponds to the following distributions of joint entropies for each dimensions: 

.. image:: images/diabetes_entropy_histograms.png

and the computation of the probability of encountering some undersampled probability density estimation (single point box) as a function of 
the dimension gives: 

.. image:: images/diabetes_undersampling.png

Which imposing an arbitrary confidence of P>0.05 (default value of the "p_value_undersmapling" parametter), gives a undersampling dimension 
:math:`k_u=6`, meaning that with such level of confidence one should not interpret the landscapes and information estimations (whatever) 
above the 5th dimension. This method is very basic and can (or shall) be improved in several ways, notably a strategy exploring undersampling 
or information paths should provide more relevant methods, adapted to data structure (to be done).

The number of tuples (a total of :math:`2^{10})`) to represent becomes to hudge, and enforces to plot only the distribution histograms of k-tuples 
value (with a given number of bins = nb_bins_histo) in each dimension. We already see that there exist some interesting structures since the
distribution  of :math:`H_3,H_4,H_5` display obvious bi-modality: the minimum joint entropy mode of the tuples contains the tuples the 
furthest from randomness. The result shows for example that the 3 first minimum 4-entropy (figure below) contains the binary "sex" variable.
It points out one of the current possible limitation-bias of the present algorithm: for heterogeneous variable input, the algorithm should 
allow different number of values adapted for each variable (binary ternary etc... at the moment their all the same... to be done).

.. image:: images/diabetes_3min_H4.png


Total correlation
~~~~~~~~~~~~~~~~~

We can now focus on the statistical depencies and :math:`G_k` and :math:`I_k` structures, we will first compute the total correlation :math:`G_k`,
by running as previously the commands:

.. code:: python3

    Ntotal_correlation = information_topo.total_correlation_simplicial_lanscape(Nentropie)
    dico_max, dico_min = information_topo.display_higher_lower_information(Ntotal_correlation, dataset)
    
and we obtain the following :math:`G_k` landscape:

.. image:: images/diabetes_total_correlation_landscape.png

which corresponds to the following distributions of free energy :math:`G_k` for each dimensions: 

.. image:: images/diabetes_total_correlation_histograms.png

The structure of dependences appears much richer, notably the landscape exhibits nice and clearcut bimodal distribution of free energy from
dimension 3 to dimension 8. The data points 4-subspace corresponding to the two first minima and maxima of :math:`G_4` look like this : 

.. image:: images/diabetes_min_max_G4.png

As expected the two :math:`G_4` minima present the dependent 4-subspace, but the the two :math:`G_4` maxima, for the 4-tuples (5,6,7,8) 
and (5,6,8,9), present higly dependent very nice statistical dependencies (further detailed in the :math:`I_4` subsection bellow).


Mutual Information
~~~~~~~~~~~~~~~~~~
We can now plot similarly the :math:`I_k` landscape, using the commands:

.. code:: python3

    Ninfomut = information_topo.simplicial_infomut_decomposition(Nentropie) 
    information_topo.mutual_info_simplicial_lanscape(Ninfomut)
    dico_max, dico_min = information_topo.display_higher_lower_information(Ninfomut, dataset)
    adjacency_matrix_mut_info =information_topo.mutual_info_pairwise_network(Ntotal_correlation)

and we obtain the following :math:`I_k` landscape:

.. image:: images/diabetes_information_landscape.png

which corresponds to the following distributions of k-mutual information for each dimensions: 

.. image:: images/diabetes_information_histograms.png

:math:`I_k` landscape bring new results that could not be infered from total correlations, notably thanks to its possible negativity.
The :math:`I_k` landscape of diabetes dataset notably displays important negative values (it was chosen to illustrate this very peculiar phenomena)
in dimension 3 and 4 for  some 3-tuples and 1 4-tuples (framed in blue). The data points 4-subspace corresponding to this minimal :math:`I_4` 
and the  maximal :math:`I_4` look like this (with different views) : 

.. image:: images/diabetes_min_max_I4.png

The tuple maximal :math:`I_4` (framed in red) only display a weak correlation, as expected from the low :math:`I_4` value. However the tuple with
minimal :math:`I_4` (5,6,7,8) displays an impressive correlation structure taking the form of a 3 dimensional hyperplane (sligtly curved indeed). 
Looking at projections on 2 dimensional subpaces as shown on the 3 plots on the right we see that the subspace corresponding to the tuples (5,6) 
and (7,8) is higly "correlated" while  (6,7) and (5,7) are highly "random". Indeed, the tuples (5,6), (7,8) and (6,8) obtain the maximum pairwise mutual 
information. This phenomena of information negativity is known in neuroscience as synergy since the work of `Brenner et al <https://arxiv.org/abs/physics/9902067>`_.
The fact that the 4-tuplet (5,6,7,8) have minimal and not maximal :math:`I_4` provides us important additional information that cannot be deduced 
form the pairwise :math:`I_2` (e.g the fact that (5,6) and (7,8) have maximum :math:`I_2`): the fact that the pair of variables  (5,6) and (7,8) and (6,8) untertain 
causal relationship but have a common cause (another, possibly joint, variable). More precisely we can infer the following causal scheme: 
:math:`5 \rightarrow 6   \leftrightarrow 8  \leftarrow 7`  (with an ambiguity in the causal dierction between 6 and 8 that could be disambiguated by having 
a look in the higher dimension 5, and an ambiguity in the global flow, all the arrows could be reversed, that could be desambiguated by looking at lower dimensions). 
This is indeed equivalent to strong transfer entropy (or conditional mutual information, see `Schreiber <https://arxiv.org/abs/nlin/0001042>`_) but applied here in a general 
context without time series structure assumption. Transfer entropy is well known to generalize Granger causality to non-linear cases 
(see `Barnet et al <https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.103.238701>`_). The classical example of a common causal variable is 
given   by: "as ice cream sales increase, the rate of drowning deaths increases sharply.": both are correlated but none of each causes the other.
A section in "how_infotopo_works" is dedicated to a more complete study and explanation of these statistical interactions. The gene expression study 
of `Tapia et al. <https://www.nature.com/articles/s41598-018-31765-z>`_ provides further examples of strong positive k-tuplet, e.g of statistical 
interactions without common cause, or more simply causal chains (e.g metabolic chains). 
The possiblity to extract causal relation from information structures, :math:`I_k` landscape, is better illustrated by analysing the 
`LUCAS0 Medical diagnosis dataset <http://www.causality.inf.ethz.ch/data/LUCAS.html>`_ sympathicaly proposed by the
`Causality Challenge #1: Causation and Prediction <http://www.causality.inf.ethz.ch/challenge.php?page=datasets>`_ . It can be acheived
by setting the variable dataset_type == 4 in the main of the python script after dowloading the csv at the previous link. 
In this synthetic training example the 3 variables "smoking", "genetics" and "lung cancer" (1,5,12) are among the minimal :math:`I_3` 
while they were designed  to exemplify the causal structure math:`1 \rightarrow 12 \leftarrow 5`. The dataset and causality results are detailed in the next section. 


Information Networks
~~~~~~~~~~~~~~~~~~~~

The information networks representation of :math:`I_1` and :math:`I_2` for the diabetes dataset is:  

.. image:: images/diabetes_information_networks.png

The maxima of :math:`I_2` are for (5,6) then (7,8) then (6,8) and minima of :math:`I_3` are for (5,7,8) then (6,7,8), and this indicate that 5 may 
cause 7 and 8, and that 6 causes 7 and 8, while 5 and 6 are highly inter-dependent, among other relation, potentially complex relationships that
can be infered from the information landscape. 


Mean Information path
~~~~~~~~~~~~~~~~~~~~~

It is interesting to compute and plot the mean :math:`I_k` paths, which consist in dividing the sum of :math:`I_k` by the binomial coefficient 
:math:`\binom{n}{k}`, and the Mean :math:`I_k` rate , which consist in dividing the preceeding result by the dimension:

.. math::	
    \langle I_k \rangle = \frac{\sum_{T\subset [n];card(T)=i}I_k(X_T;P)}{\binom{n}{k}}


Using the command:   

.. code:: python3

    mean_info, mean_info_rate = information_topo.display_mean_information(Ninfomut)

we obtain the following  mean :math:`I_k` paths and   mean :math:`I_k` rate paths: 

.. image:: images/diabetes_mean_Ik.png

Mean :math:`I_k` corresponds to the mean-field approxiamtion in statistical physics, that assumes a homogeneous system with identical particles 
and identical k-body interactions. We recover a usual free-energy landscape analogous to n-bdy van der Waals model, here with a (little) minima 
at the critical dimension 3, which shows that the interactions (or statistical dependences) in the data are weak in average (almost the 
independent case). The same computation and definitions can be acheived for k-entropy, and is let as an exercise. 

Conditional (transfer) Informations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The visualization of information landscapes as histograms do not permit to visualize and study the conditional entropies and Mutual informations, 
that can be very interesting as we saw with the (extension) of transfer entropy. They are given by chain rules and correspond to minus the slope 
of each edges of the lattice in the landscapes. It is possible to plot them using the command: 

.. code:: python3

    NcondInfo = information_topo.conditional_info_simplicial_lanscape(Ninfomut)
    information_topo.display_higher_lower_cond_information(NcondInfo)

There are more conditional Informations than :math:`I_k` (:math:`k\binom{n}{k}` in each k-dimension, and :math:`n2^{n-1}` in total), and we 
encoded the output as a list for each dimension, "NcondInfo", of dictionaries which items are of the forms ((5, 7, 9), 0.352)  for 
the information of 5,7 knowing 9, e.g. I(5,7|9). Indeed, as remarked by (`Han (1981) <https://www.researchgate.net/publication/268827547_A_uniqueness_of_Shannon%27s_information_distance_and_related_nonnegativity_problems>`_ 
`Yeung <http://iest2.ie.cuhk.edu.hk/~whyeung/post/draft2.pdf>`_ generates
all the other information quantities we saw: considering the conditionning variable as the deterministic unit we obtain mutual informations, and
considering equivalent variables we obtain conditional entropies and entropies. Both the "Shannonian" and "non-shannonian" inequalities found by
Yeung translates directly in information landscapes as bounds on the slope paths (or topological cones), unraveling their homological nature 
(see `PDF <https://www.mdpi.com/1099-4300/21/9/881>`_). For the diabetes dataset, we obtain:

.. image:: images/diabetes_condinfo_landscape.png

Entropy Vs. Energy
~~~~~~~~~~~~~~~~~~

Following the `original figure <https://en.wikipedia.org/wiki/Gibbs_free_energy>`_ ENTROPY vs. ENERGY vs. VOLUME of Willard Gibbs (1873) James 
Clerk Maxwell (1874), we can resume part of the preceding results by ploting :math:`H_k` (absyssa) vs. :math:`G_k` (ordinate) using the command:

.. code:: python3

    information_topo.display_entropy_energy_landscape(Ntotal_correlation, Nentropie)

.. image:: images/diabetes_entropy_energyGk_landscape.png

It notably shows how two population of data points clusters from dimension 6 to 8. 


Information distance
~~~~~~~~~~~~~~~~~~~~

Another nice information measure is information distance or metric defined by :math:`V_2(X;Y) =H_2(X,Y)-I_2(X;Y)` . It is a "real" (and unique see `Han for unicity proof <https://www.researchgate.net/publication/268827547_A_uniqueness_of_Shannon%27s_information_distance_and_related_nonnegativity_problems>`_
metric in the sens that it satifies triangle inequalities and symmetry (precisely except identity if null, it is even better than a metric, it is a pseudo-metric). This metric was 
find by  `Shannon (1953) <https://ieeexplore.ieee.org/abstract/document/1188572>`_ ,
and was the subject of further interesting both applied and theoretical studies (`Han 1981 <https://www.researchgate.net/publication/268827547_A_uniqueness_of_Shannon%27s_information_distance_and_related_nonnegativity_problems>`_ ,  
`Rajski 1961 <https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwj77P3Cp9DrAhUNExQKHfZSAxUQFjAAegQIBRAB&url=https%3A%2F%2Fcore.ac.uk%2Fdownload%2Fpdf%2F82383189.pdf&usg=AOvVaw2WmOW58ouwhVMBNifqz4ej>`_  
, `Zurek <https://www.nature.com/articles/341119a0>`_ , `Bennett <https://arxiv.org/abs/1006.3520>`_ and   
`Kraskov and Grassberger <https://www.researchgate.net/publication/1773919_MIC_Mutual_Information_Based_Hierarchical_Clustering>`_).  It indeed appears as a 
topological invariant in a precise setting  cohomological setting and generalises to the multivariate case to k information volumes
:math:`V_k =H_k(X,Y)-I_k(X;Y)`  `PDF <https://www.mdpi.com/1099-4300/21/9/881>`_ .  :math:`V_k` are non-neagtive and symmetric functions. For Machine Learning,
this shall be understood as an informational version of Jaccard metric, intersection over union (iou) or other union minus intersection metrics.
We can compute their simplicial structure using the commands:

.. code:: python3

    Ninfo_volume = information_topo.information_volume_simplicial_lanscape(Nentropie, Ninfomut)
    dico_max, dico_min = information_topo.display_higher_lower_information(Ninfo_volume, dataset)
    adjacency_matrix_info_distance = information_topo.mutual_info_pairwise_network(Ninfo_volume)

On the Diabete dataset, it gives the following :math:`V_k` landscape: 

.. image:: images/diabetes_info_volume_landscape.png

with the following distributions:

.. image:: images/diabetes_info_volume_histograms.png 

We see that the structure is less interesting compared to the one we obtained with :math:`I_k` and :math:`G_k`, but its geometrical status of a 
(pseudo)-metric leaves it appealing to plot in its network representation.
 
Beware that these tools will not detect whatever possible statistical dependencies (see James and Crutchfield `PDF <https://www.mdpi.com/1099-4300/19/10/531>`_), 
this is just a simplicial subset (nice...paths are automorphism) subsets, computationnally tractable. The complete structure of dependencies are spanned by general information structures and 
lattice of patition (see section how_infotopo_works), which embedds the present simplicial case. 
This concludes our introduction to basic infotopo usage -- hopefully this
has given you the tools to get started for yourself. Further tutorials,
covering infotopo parameters and more advanced usage are also available when
you wish to dive deeper.(X)